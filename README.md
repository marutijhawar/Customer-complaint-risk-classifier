# Customer Complaint Risk Classifier ğŸ¯

An end-to-end NLP system that classifies customer complaint risk levels to identify high-priority cases requiring immediate attention. Optimized for lightweight execution on consumer hardware.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [How It Works](#how-it-works)
- [Performance](#performance)
- [Configuration](#configuration)
- [Output](#output)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Overview

This system processes unstructured customer complaint text to predict escalation risk levels (Low, Medium, High) with a focus on maximizing recall for high-risk cases to prevent customer churn.

**Key Benefits:**
- ğŸš€ Fast: Runs in 1-2 minutes on consumer hardware
- ğŸ’¾ Lightweight: ~2-3GB peak memory usage
- ğŸ¯ Accurate: 80-85% overall accuracy, 85-90% high-risk recall
- ğŸ“Š Interpretable: Feature importance and explainability built-in
- ğŸ’° Business-focused: ROI and churn cost analysis included

## âœ¨ Features

### Core Capabilities
- âœ… **Text Preprocessing**: Cleaning, tokenization, stopword removal
- âœ… **Feature Engineering**: TF-IDF, sentiment analysis, text statistics
- âœ… **Multi-Model Training**: Logistic Regression, Random Forest, XGBoost, SVM
- âœ… **Performance Optimization**: Class balancing, threshold tuning for high recall
- âœ… **Model Explainability**: Feature importance visualization
- âœ… **Business Analytics**: Churn cost estimation and ROI analysis

### Technical Features
- Optimized for MacBook Air M3 / Similar consumer hardware
- Configurable via YAML (no code changes needed)
- Synthetic data generation for testing
- Comprehensive evaluation metrics
- Automated visualization generation

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Step 1: Clone Repository
```bash
git clone https://github.com/yourusername/customer-complaint-risk-classifier.git
cd customer-complaint-risk-classifier
```

### Step 2: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 3: Download NLTK Data
```bash
python -c "import nltk; nltk.download('punkt_tab'); nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('vader_lexicon')"
```

## ğŸƒ Quick Start

Run the complete pipeline:

```bash
python main_lite.py
```

**Expected Output:**
```
============================================================
CUSTOMER COMPLAINT RISK CLASSIFICATION SYSTEM
(Lightweight Version for MacBook Air M3)
============================================================

[1/6] Loading Data...
[2/6] Preprocessing Text...
[3/6] Splitting Data...
[4/6] Engineering Features...
[5/6] Training Models...
[6/6] Evaluating Models...

Pipeline Complete!
Best Model: [Model Name]
```

## ğŸ“ Project Structure

```
customer-complaint-risk-classifier/
â”‚
â”œâ”€â”€ main_lite.py              # Main pipeline entry point
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml           # Configuration settings
â”‚
â”œâ”€â”€ src/                      # Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py        # Data loading & generation
â”‚   â”œâ”€â”€ preprocessing.py      # Text preprocessing
â”‚   â”œâ”€â”€ feature_engineering.py # Feature extraction
â”‚   â”œâ”€â”€ models.py             # Model training
â”‚   â”œâ”€â”€ evaluation.py         # Performance evaluation
â”‚   â””â”€â”€ explainability.py     # Model interpretability
â”‚
â”œâ”€â”€ data/                     # Data storage
â”‚   â”œâ”€â”€ raw/                  # Original data
â”‚   â””â”€â”€ processed/            # Preprocessed data
â”‚
â”œâ”€â”€ models/                   # Saved models
â”‚
â””â”€â”€ reports/                  # Outputs
    â””â”€â”€ figures/              # Visualizations
```

## ğŸ”§ How It Works

### 1. Data Loading
- Loads existing CSV or generates synthetic complaint data
- 300 samples with realistic complaint narratives
- Balanced distribution: ~60% Low, ~25% Medium, ~15% High risk

### 2. Text Preprocessing
- URL, email, phone number removal
- Lowercase conversion
- Stopword removal (keeping sentiment indicators)
- Text normalization

### 3. Feature Engineering
- **TF-IDF Features**: 200 most important unigrams
- **Sentiment Analysis**: VADER sentiment scores (neg, neu, pos, compound)
- **Text Statistics**: Length, word count, punctuation, caps ratio
- **Domain Features**: Urgency words, negative words, escalation indicators

### 4. Model Training
Four models trained with 3-fold cross-validation:
- Logistic Regression (baseline)
- Random Forest (ensemble)
- XGBoost (gradient boosting)
- SVM (support vector machine)

All models use class balancing to handle imbalanced data.

### 5. Evaluation
- Standard metrics: Accuracy, Precision, Recall, F1, ROC AUC
- High-risk focus: Optimized threshold for 90% recall
- Confusion matrix analysis
- Business impact estimation

### 6. Explainability
- Feature importance ranking
- Top contributing features for risk prediction

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| Overall Accuracy | 80-85% |
| High-Risk Recall | 85-90% |
| High-Risk Precision | 70-80% |
| Training Time | 30-60 seconds |
| Total Runtime | 1-2 minutes |
| Peak Memory | 2-3 GB |

**Hardware Tested:** MacBook Air M3 16GB RAM

## âš™ï¸ Configuration

Edit `config/config.yaml` to customize:

### Data Settings
```yaml
data:
  test_size: 0.2          # Train/test split ratio
  random_state: 42        # Reproducibility seed
```

### Feature Settings
```yaml
features:
  tfidf:
    max_features: 200     # Number of TF-IDF features
    ngram_range: [1, 1]   # Unigrams only (increase for bigrams)
  sentiment:
    include_vader: true   # Enable/disable VADER sentiment
```

### Model Settings
```yaml
models:
  random_forest:
    n_estimators: [30]    # Number of trees
    max_depth: [8]        # Tree depth
```

### Performance Tuning

**For faster execution** (lower accuracy):
- Reduce `max_features` to 100
- Reduce `n_estimators` to 20
- Disable SVM (slowest model)

**For better accuracy** (slower, more memory):
- Increase `max_features` to 500
- Change `ngram_range` to [1, 2] (add bigrams)
- Increase `n_estimators` to 50-100
- Set `lemmatize: true` (enables spaCy)

## ğŸ“¤ Output

### Files Generated

1. **Data Files**
   - `data/raw/complaints.csv` - Complaint data (300 samples)
   - `data/processed/complaints_processed.csv` - Preprocessed text

2. **Model Files**
   - `models/best_model.joblib` - Trained classifier

3. **Visualizations**
   - `reports/figures/confusion_matrix.png` - Prediction accuracy matrix
   - `reports/figures/precision_recall.png` - PR curve for high-risk detection
   - `reports/figures/roc_curves.png` - ROC curves for all classes
   - `reports/figures/feature_importance.png` - Top predictive features

### Console Output

- Class distribution statistics
- Feature engineering summary
- Model training progress
- Performance metrics
- High-risk detection analysis
- Business impact estimation (churn cost saved)

## ğŸ› Troubleshooting

### Issue: Code running slowly
**Solution:**
- Reduce sample size: Change `n_samples=300` to `200` in `main_lite.py` line 46
- Reduce features: Set `max_features: 100` in `config/config.yaml`
- Close other applications to free RAM

### Issue: Memory errors
**Solution:**
- Reduce `max_features` to 100
- Reduce `n_samples` to 150
- Disable SVM model (comment out in models.py)

### Issue: ImportError or NLTK errors
**Solution:**
```bash
pip install -r requirements.txt --upgrade
python -c "import nltk; nltk.download('all')"
```

### Issue: Low accuracy
**Solution:**
- Increase `max_features` to 500
- Add bigrams: `ngram_range: [1, 2]`
- Increase `n_estimators` to 50
- Enable lemmatization: `lemmatize: true`

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **scikit-learn** for machine learning algorithms
- **XGBoost** for gradient boosting
- **NLTK** for NLP preprocessing
- **VADER** for sentiment analysis

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Built with â¤ï¸ for customer success teams**
